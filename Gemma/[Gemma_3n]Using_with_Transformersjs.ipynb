{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVp8vazXYOz-"
      },
      "source": [
        "##### Copyright 2026 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DBaXaQ_PYT4p"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSkl5h3dZMo9"
      },
      "source": [
        "# Gemma 3n - Run with Transformers.js\n",
        "\n",
        "Author: Sitam Meur\n",
        "\n",
        "*   GitHub: [github.com/sitammeur](https://github.com/sitammeur/)\n",
        "*   X: [@sitammeur](https://x.com/sitammeur)\n",
        "\n",
        "Description: This notebook demonstrates how you can run inference on Gemma 3n model using Node.js and [Transformers.js](https://huggingface.co/docs/transformers.js/index). Transformers.js lets you run Hugging Face's transformer models directly in browser, offering a JavaScript API similar to Python's.  It supports NLP, computer vision, audio, and multimodal tasks using ONNX Runtime and allows easy conversion of PyTorch, TensorFlow, and JAX models.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/[Gemma_3n]Using_with_Transformersjs.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftkrrn3aZyAl"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Select the Colab runtime\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma 3n model. In this case, you can use CPU runtime:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **â–¾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **CPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCJ7yo3-Zzdj"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET_KH77YZ5lc"
      },
      "source": [
        "Let's get started with installing the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7Ds4h2q0ItU"
      },
      "outputs": [],
      "source": [
        "# Install Node.js\n",
        "!curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -\n",
        "!sudo apt-get install -y nodejs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObI8d_Rwa_nn"
      },
      "source": [
        "## Create Node.js project\n",
        "\n",
        "Create a new Node.js project and install the required transformers package via [NPM](https://www.npmjs.com/package/@huggingface/transformers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nmPcg5J0cYj"
      },
      "outputs": [],
      "source": [
        "# Create project directory\n",
        "!mkdir gemma3n-node\n",
        "%cd gemma3n-node\n",
        "\n",
        "# Initialize NPM project\n",
        "!npm init -y\n",
        "!npm i @huggingface/transformers wavefile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGYVoOXA3T7-"
      },
      "outputs": [],
      "source": [
        "%%writefile package.json\n",
        "\n",
        "{\n",
        "  \"name\": \"gemma3n-node\",\n",
        "  \"version\": \"1.0.0\",\n",
        "  \"main\": \"index.js\",\n",
        "  \"type\": \"module\",\n",
        "  \"scripts\": {\n",
        "    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n",
        "  },\n",
        "  \"keywords\": [],\n",
        "  \"author\": \"\",\n",
        "  \"license\": \"ISC\",\n",
        "  \"description\": \"\",\n",
        "  \"dependencies\": {\n",
        "    \"@huggingface/transformers\": \"^3.8.1\",\n",
        "    \"wavefile\": \"^11.0.0\"\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9mJ6Pi3bxCY"
      },
      "source": [
        "## Transformers.js Inference\n",
        "\n",
        "Now, let's run inference on the Gemma 3n model using Transformers.js. First, create a generation pipeline for images and text, audio and text, and images and audio. Then, prepare the inputs to run inference and obtain the desired output. For reference, you can check the model's page on the Hugging Face model hub under ONNX models section [here](https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yk46lGdN0sl"
      },
      "source": [
        "### Image + Text Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADScBVbB1cwW"
      },
      "outputs": [],
      "source": [
        "# Show the image from the URL\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://jethac.github.io/assets/juice.jpg\"\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Gy2LdaY3Iqh"
      },
      "outputs": [],
      "source": [
        "%%writefile index.js\n",
        "\n",
        "// Import the required modules\n",
        "import {\n",
        "  AutoProcessor,\n",
        "  AutoModelForImageTextToText,\n",
        "  load_image,\n",
        "  TextStreamer,\n",
        "} from \"@huggingface/transformers\";\n",
        "\n",
        "// Load processor and model\n",
        "const model_id = \"onnx-community/gemma-3n-E2B-it-ONNX\";\n",
        "const processor = await AutoProcessor.from_pretrained(model_id);\n",
        "const model = await AutoModelForImageTextToText.from_pretrained(model_id, {\n",
        "  dtype: {\n",
        "    embed_tokens: \"q8\",\n",
        "    audio_encoder: \"q8\",\n",
        "    vision_encoder: \"fp16\",\n",
        "    decoder_model_merged: \"q4\",\n",
        "  },\n",
        "  device: \"cpu\",\n",
        "});\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: [\n",
        "      { type: \"image\" },\n",
        "      { type: \"text\", text: \"Describe this image in detail.\" },\n",
        "    ],\n",
        "  },\n",
        "];\n",
        "\n",
        "try {\n",
        "  // Prepare prompt\n",
        "  const prompt = processor.apply_chat_template(messages, {\n",
        "    add_generation_prompt: true,\n",
        "  });\n",
        "\n",
        "  // Prepare inputs\n",
        "  const url = \"https://jethac.github.io/assets/juice.jpg\";\n",
        "  const image = await load_image(url);\n",
        "  const audio = null;\n",
        "\n",
        "  const inputs = await processor(prompt, image, audio, {\n",
        "    add_special_tokens: false,\n",
        "  });\n",
        "\n",
        "  // Generate output\n",
        "  const outputs = await model.generate({\n",
        "    ...inputs,\n",
        "    max_new_tokens: 512,\n",
        "    do_sample: false,\n",
        "  });\n",
        "\n",
        "  // Decode output\n",
        "  const promptLen = inputs.input_ids.dims.at(-1);\n",
        "  const decoded = processor.batch_decode(outputs.slice(null, [promptLen, null]), {\n",
        "    skip_special_tokens: true,\n",
        "  });\n",
        "\n",
        "  console.log(decoded[0]);\n",
        "} catch (error) {\n",
        "  console.error(\"Error generating response:\", error);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmVUVFDf3-yE"
      },
      "outputs": [],
      "source": [
        "# Run the node.js application (Image + Text)\n",
        "!node index.js"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKeWNMVvOEET"
      },
      "source": [
        "### Audio + Text Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_Wvd4-F6QPk"
      },
      "outputs": [],
      "source": [
        "# Display audio\n",
        "from IPython.display import Audio\n",
        "import requests\n",
        "import io\n",
        "\n",
        "url = \"https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav\"\n",
        "\n",
        "audio_bytes = requests.get(url).content\n",
        "Audio(audio_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNsqIEc1-swY"
      },
      "outputs": [],
      "source": [
        "%%writefile index.js\n",
        "\n",
        "// Import the required modules\n",
        "import {\n",
        "  AutoProcessor,\n",
        "  AutoModelForImageTextToText,\n",
        "  TextStreamer,\n",
        "} from \"@huggingface/transformers\";\n",
        "import wavefile from \"wavefile\";\n",
        "\n",
        "// Load processor and model\n",
        "const model_id = \"onnx-community/gemma-3n-E2B-it-ONNX\";\n",
        "const processor = await AutoProcessor.from_pretrained(model_id);\n",
        "const model = await AutoModelForImageTextToText.from_pretrained(model_id, {\n",
        "  dtype: {\n",
        "    embed_tokens: \"q8\",\n",
        "    audio_encoder: \"q4\",\n",
        "    vision_encoder: \"fp16\",\n",
        "    decoder_model_merged: \"q4\",\n",
        "  },\n",
        "  device: \"cpu\",\n",
        "});\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: [\n",
        "      { type: \"audio\" },\n",
        "      { type: \"text\", text: \"Transcribe this audio.\" },\n",
        "    ],\n",
        "  },\n",
        "];\n",
        "\n",
        "try {\n",
        "  // Prepare prompt\n",
        "  const prompt = processor.apply_chat_template(messages, {\n",
        "    add_generation_prompt: true,\n",
        "  });\n",
        "\n",
        "  // Prepare inputs (audio from URL -> Float32Array @ model sample rate)\n",
        "  const url =\n",
        "    \"https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav\";\n",
        "\n",
        "  const buffer = Buffer.from(await fetch(url).then((x) => x.arrayBuffer()));\n",
        "  const wav = new wavefile.WaveFile(buffer);\n",
        "\n",
        "  // Pipeline expects Float32 samples\n",
        "  wav.toBitDepth(\"32f\");\n",
        "  wav.toSampleRate(processor.feature_extractor.config.sampling_rate);\n",
        "\n",
        "  let audioData = wav.getSamples();\n",
        "\n",
        "  // Convert stereo -> mono (simple average with sqrt(2) normalization, like the docs)\n",
        "  if (Array.isArray(audioData)) {\n",
        "    if (audioData.length > 1) {\n",
        "      for (let i = 0; i < audioData[0].length; ++i) {\n",
        "        audioData[0][i] =\n",
        "          (Math.sqrt(2) * (audioData[0][i] + audioData[1][i])) / 2;\n",
        "      }\n",
        "    }\n",
        "    audioData = audioData[0];\n",
        "  }\n",
        "\n",
        "  const image = null;\n",
        "  const audio = audioData;\n",
        "\n",
        "  const inputs = await processor(prompt, image, audio, {\n",
        "    add_special_tokens: false,\n",
        "  });\n",
        "\n",
        "  // Generate output\n",
        "  const outputs = await model.generate({\n",
        "    ...inputs,\n",
        "    max_new_tokens: 512,\n",
        "    do_sample: false,\n",
        "  });\n",
        "\n",
        "  // Decode output\n",
        "  const promptLen = inputs.input_ids.dims.at(-1);\n",
        "  const decoded = processor.batch_decode(outputs.slice(null, [promptLen, null]), {\n",
        "    skip_special_tokens: true,\n",
        "  });\n",
        "\n",
        "  console.log(decoded[0]);\n",
        "} catch (error) {\n",
        "  console.error(\"Error generating response:\", error);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqlFYl9B--2V"
      },
      "outputs": [],
      "source": [
        "# Run the node.js application (Audio + Text)\n",
        "!node index.js"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbeZ9LQWOKEi"
      },
      "source": [
        "### Image + Audio Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypM4UJVGOZuZ"
      },
      "outputs": [],
      "source": [
        "# Show the image from the URL\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://jethac.github.io/assets/juice.jpg\"\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0szDcubrKwLr"
      },
      "outputs": [],
      "source": [
        "# Display audio\n",
        "from IPython.display import Audio\n",
        "import requests\n",
        "import io\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/sitammeur/test-assets/main/cat.wav\"\n",
        "\n",
        "audio_bytes = requests.get(url).content\n",
        "Audio(audio_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB5Z18H7JaDL"
      },
      "outputs": [],
      "source": [
        "%%writefile index.js\n",
        "\n",
        "// Import the required modules\n",
        "import {\n",
        "  AutoProcessor,\n",
        "  AutoModelForImageTextToText,\n",
        "  load_image,\n",
        "  TextStreamer,\n",
        "} from \"@huggingface/transformers\";\n",
        "import wavefile from \"wavefile\";\n",
        "\n",
        "// Load processor and model\n",
        "const model_id = \"onnx-community/gemma-3n-E2B-it-ONNX\";\n",
        "const processor = await AutoProcessor.from_pretrained(model_id);\n",
        "const model = await AutoModelForImageTextToText.from_pretrained(model_id, {\n",
        "  dtype: {\n",
        "    embed_tokens: \"q8\",\n",
        "    audio_encoder: \"q4\",\n",
        "    vision_encoder: \"fp16\",\n",
        "    decoder_model_merged: \"q4\",\n",
        "  },\n",
        "  device: \"cpu\",\n",
        "});\n",
        "\n",
        "// Define the list of messages\n",
        "const messages = [\n",
        "  {\n",
        "    role: \"user\",\n",
        "    content: [{ type: \"image\" }, { type: \"audio\" }],\n",
        "  },\n",
        "];\n",
        "\n",
        "try {\n",
        "  // Prepare prompt\n",
        "  const prompt = processor.apply_chat_template(messages, {\n",
        "    add_generation_prompt: true,\n",
        "  });\n",
        "\n",
        "  // Prepare inputs (image + audio from URLs)\n",
        "  const imageUrl = \"https://jethac.github.io/assets/juice.jpg\";\n",
        "  const image = await load_image(imageUrl);\n",
        "\n",
        "  const audioUrl = \"https://raw.githubusercontent.com/sitammeur/test-assets/main/cat.wav\";\n",
        "  const buffer = Buffer.from(await fetch(audioUrl).then((r) => r.arrayBuffer()));\n",
        "  const wav = new wavefile.WaveFile(buffer);\n",
        "\n",
        "  // Pipeline expects Float32 samples at model sample rate\n",
        "  wav.toBitDepth(\"32f\");\n",
        "  wav.toSampleRate(processor.feature_extractor.config.sampling_rate);\n",
        "\n",
        "  let audioData = wav.getSamples();\n",
        "\n",
        "  // Convert stereo -> mono\n",
        "  if (Array.isArray(audioData) && audioData.length > 1) {\n",
        "    const left = audioData[0];\n",
        "    const right = audioData[1];\n",
        "    audioData = left.map((_, i) => (Math.sqrt(2) * (left[i] + right[i])) / 2);\n",
        "  } else if (Array.isArray(audioData)) {\n",
        "    // If it's an array wrapper (single channel), unwrap it\n",
        "    audioData = audioData[0];\n",
        "  }\n",
        "\n",
        "  const inputs = await processor(prompt, image, audioData, {\n",
        "    add_special_tokens: false,\n",
        "  });\n",
        "\n",
        "  // Generate output\n",
        "  const outputs = await model.generate({\n",
        "    ...inputs,\n",
        "    max_new_tokens: 512,\n",
        "    do_sample: false,\n",
        "  });\n",
        "\n",
        "  // Decode output\n",
        "  const promptLen = inputs.input_ids.dims.at(-1);\n",
        "  const decoded = processor.batch_decode(outputs.slice(null, [promptLen, null]), {\n",
        "    skip_special_tokens: true,\n",
        "  });\n",
        "\n",
        "  console.log(decoded[0]);\n",
        "} catch (error) {\n",
        "  console.error(\"Error generating response:\", error);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QGOzHUXJr3F"
      },
      "outputs": [],
      "source": [
        "# Run the node.js application (Image + Audio)\n",
        "!node index.js"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFgOMvLmcVnY"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You have successfully run inference on Gemma 3n model using Transformers.js via Node.js environment. You can now integrate this into your projects."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "[Gemma_3n]Using_with_Transformersjs.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
