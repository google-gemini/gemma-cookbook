{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f705f4be70e9"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbabcabf26d3"
      },
      "source": [
        "# Using PEFT (parameter efficient fine-tuning technique) with huggingface to fine-tune Gemma model\n",
        " \n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/%5BGemma_2%5DFinetune_with_LORA.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/%5BGemma_2%5DFinetune_with_LORA.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c332a96cab6"
      },
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| [Shivam Ghuge](https://github.com/Shiv-am-04) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01d9f53a79b0"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to fine-tune LLM and SLM using the PEFT (parameter efficient finetuning technique) which is LORA (Low Rank Adaptation) in our case.\n",
        "\n",
        "### Objective\n",
        "\n",
        "The Goal is to use fine-tune the model in the environment where we have less compute resources like smaller GPUs, less RAM and less storage. We are fine-tuning google's open source gemma2 model using LORA technique.\n",
        "\n",
        "**We will cover the following steps:**\n",
        "\n",
        "1. ***Loading Model*** : We are using huggingface to load the model in the notebook using 4-bit quantization, which leads to a smaller  model size, lower memory usage, faster inference speed, and reduced energy consumption.\n",
        "\n",
        "2. ***Configure BitsAnsBytes*** : Using bitsandbytes config to load the model from huggingface in 4-bit.\n",
        "\n",
        "3. ***Prepare the Dataset*** : Download the SQl dataset from huggingface and convert it to Huggingface Dataset.\n",
        "\n",
        "4. ***Perform fine-tuning*** : Using LORA to do the fine-tuning of the model on the dataset\n",
        "\n",
        "5. ***Deploy*** : Push the model to the huggingface hub from where we can use it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qprM8Fl5L2xh"
      },
      "source": [
        "#### ***Install PEFT (parameter efficient fine tuning), bitsandbytes and other required packages***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b3c941159c2"
      },
      "outputs": [],
      "source": [
        "%pip install peft bitsandbytes transformers accelerate datasets trl google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c5ab12a4af5"
      },
      "outputs": [],
      "source": [
        "# import tensorflow\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM,BitsAndBytesConfig,TrainingArguments,logging\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb5PfLChPL6f"
      },
      "source": [
        "#### ***BitsAndBytes Configuration***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cf119462d65d"
      },
      "outputs": [],
      "source": [
        "### bitsandbytes parameters ###\n",
        "\n",
        "# The bitsandbytes library is a lightweight Python wrapper around CUDA custom functions, particularly designed for 8-bit optimizers,matrix multiplication (LLM.int8()), and 8-bit and 4-bit quantization functions\n",
        "\n",
        "bnb4bit_compute_dtype = 'float16'\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "# fp4 : A standard, 4-bit floating-point format that uses a 1-bit sign, a 2-bit exponent, and a 1-bit mantissa.\n",
        "# nf4 : Same as fp4 but it is normalized 4-bit and optimized for normally distributed data like the weights in large language model.\n",
        "#       This makes it more efficient for training and inference of LLM models.\n",
        "bnb4bit_quant_type = 'nf4'\n",
        "\n",
        "use_nested_quant = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c5467e721e9b"
      },
      "outputs": [],
      "source": [
        "# fetch the value of bnb4bit_compute_dtype from the torch module.\n",
        "\n",
        "compute_dtype = getattr(torch,bnb4bit_compute_dtype)\n",
        "\n",
        "# getattr is a built-in Python function that retrieves an attribute from an object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9c5afc65d3d1"
      },
      "outputs": [],
      "source": [
        "bitsAndbytes_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                         bnb_4bit_compute_dtype=compute_dtype,\n",
        "                                         bnb_4bit_quant_type=bnb4bit_quant_type,\n",
        "                                         bnb_4bit_use_double_quant=False,\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik_AmgzMPShr"
      },
      "source": [
        "#### ***Loading gemma-2-2b model from huggingface***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a25d6a6aaf0b"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "access_token = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bf35ef9d076"
      },
      "outputs": [],
      "source": [
        "model_name = 'google/gemma-2-2b'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58c2f8c6a823"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             quantization_config=bitsAndbytes_config,\n",
        "                                             device_map='auto',                                   # device_map is where to load the entire model (0:gpu,'auto':whichever available)\n",
        "                                             attn_implementation = 'eager',                       # type of self-attention technique\n",
        "                                             token=access_token)\n",
        "\n",
        "\n",
        "# Disables the use of caching during model inference.\n",
        "model.config.use_cache = False\n",
        "# Caching stores intermediate results to speed up future computations. Turning it off might be necessary if caching leads to high memory consumption\n",
        "# or isn't beneficial for our task.\n",
        "\n",
        "# Sets the degree of tensor parallelism for pretraining.\n",
        "model.config.pretraining_tp = 1\n",
        "# Tensor parallelism splits the model tensors across multiple devices (e.g., GPUs) to speed up training. A value of 1 means no tensor splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "477576e89044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2 GB\n"
          ]
        }
      ],
      "source": [
        "print(f\"{model.get_memory_footprint()/1e9:,.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f768b6ca872b"
      },
      "outputs": [],
      "source": [
        "# help(AutoModelForCausalLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCKqDhOLs_tZ"
      },
      "source": [
        "***Generating before fine-tuning***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b72f8e777fdf"
      },
      "outputs": [],
      "source": [
        "question = 'there is a table name Employee containing two columns employee_id and salary. Give me only sql query to fetch the highest and lowest salary along with employee id'\n",
        "device = 'cuda'\n",
        "input_ = tokenizer.encode(question,return_tensors='pt').to(device)\n",
        "response = model.generate(input_).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2fd2c41587e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "there is a table name Employee containing two columns employee_id and salary. Give me only sql query to fetch the highest and lowest salary along with employee id.\n",
            "\n",
            "<code>SELECT MAX(salary) AS max_salary, MIN(salary) AS min_\n"
          ]
        }
      ],
      "source": [
        "response = tokenizer.decode(response[0],skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdm7c8klZZnw"
      },
      "source": [
        "***PEFT***\n",
        "\n",
        "***Parameter-Efficient Fine-Tuning, is a technique used to adapt pre-trained language models (LLMs) for specific tasks by only training a small subset of the model's parameters. This is a much more efficient and less resource-intensive alternative to traditional fine-tuning, which would update every parameter in a large model.***\n",
        "\n",
        "***By freezing most of the original model's weights and training a small number of new or existing parameters, PEFT methods achieve comparable performance while saving significant computational power and memory.***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7mqtR45tFfH"
      },
      "source": [
        "#### ***Tuning Phase***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39bcb27bcb8d"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "27f2c6e78069"
      },
      "outputs": [],
      "source": [
        "Target_modules = ['q_proj','k_proj','v_proj','o_proj']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "be5efe0de20b"
      },
      "outputs": [],
      "source": [
        "### QLORA hyperparameters ###\n",
        "\n",
        "lora_learning_rate = 1e-4\n",
        "lora_rank = 8\n",
        "lora_dropout = 0.2\n",
        "lora_alpha = 16               # double of lora rank\n",
        "\n",
        "# even using QLORA lora config is required because LORA low rank optimization is applied after quantization and alpha should be double the rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56d07e24659e"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(r=lora_rank,\n",
        "              lora_alpha=lora_alpha,\n",
        "              lora_dropout=lora_dropout,                       # A regularization technique used during training to prevent overfitting of the small, trainable LoRA matrices.\n",
        "              bias='none',\n",
        "              task_type='CAUSAL_LM',                           # CAUSAL_LM are those model that generates text by predicting the next word (or token) in a sequence based only on the words that have come before it\n",
        "              target_modules=Target_modules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPTZ3jKIKUQ6"
      },
      "source": [
        "***Data Preparation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3ad265ba4c04"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = {'train': 'data/train-00000-of-00001-36a24700f19484dc.parquet', 'validation': 'data/validation-00000-of-00001-fa01d04c056ac579.parquet'}\n",
        "df_train = pd.read_parquet(\"hf://datasets/lamini/spider_text_to_sql/\" + splits[\"train\"])\n",
        "df_test = pd.read_parquet(\"hf://datasets/lamini/spider_text_to_sql/\" + splits[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "113cd8a16dec"
      },
      "outputs": [],
      "source": [
        "df = pd.merge(df_train,df_test,how ='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f1d33f1dc438"
      },
      "outputs": [],
      "source": [
        "def remove(row):\n",
        "  return row.split('\\n\\n')[-1].replace('[/INST]','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "e9c1b00d1cb1"
      },
      "outputs": [],
      "source": [
        "df['input'] = df['input'].apply(remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0eb078361936"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for txt,query in zip(df['input'],df['output']):\n",
        "  template = f\"<question> {txt.split(':')[-1]} , <code> {query}\"\n",
        "  data.append(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "213da0058f1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8034, 2)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "33fd52d8a170"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8034"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1df65878db37"
      },
      "outputs": [],
      "source": [
        "# we are only training on 2000 for quick training\n",
        "\n",
        "data_for_training = data[:2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "724fc72e1202"
      },
      "outputs": [],
      "source": [
        "data_for_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8f43a0d02d4d"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "pd_data = pd.DataFrame(data_for_training,columns=['text'])\n",
        "hf_dataset = Dataset.from_pandas(pd_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "793a82c61194"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 2000\n",
              "})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z1aVME3KPPU"
      },
      "source": [
        "***Training Phase***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6bf675d00e86"
      },
      "outputs": [],
      "source": [
        "### training configuration ###\n",
        "\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Batch size per GPU for training\n",
        "train_batch_size_perGPU = 1\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "eval_batch_size_perGPU = 1\n",
        "\n",
        "# Number of update steps to accumulate the gradients for if our setup can manage it, keeping it simple with 1 works fine\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Optimizer to use\n",
        "optimizer_ = \"paged_adamw_32bit\"\n",
        "\n",
        "# learning rate (AdamW optimizer), lower learning rates tend to provide more stable and gradual learning.\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate) (optional)\n",
        "warmup_ratio = 0.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9fcc754ac6db"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(output_dir=output_dir,\n",
        "                                  num_train_epochs=num_train_epochs,\n",
        "                                  per_device_train_batch_size=train_batch_size_perGPU,\n",
        "                                  per_device_eval_batch_size=eval_batch_size_perGPU,\n",
        "                                  gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "                                  optim=optimizer_,\n",
        "                                  save_steps=0,\n",
        "                                  logging_steps=25,\n",
        "                                  learning_rate=learning_rate,\n",
        "                                  weight_decay=weight_decay,\n",
        "                                  fp16=False,\n",
        "                                  bf16=True,\n",
        "                                  max_grad_norm=max_grad_norm,\n",
        "                                  max_steps=max_steps,\n",
        "                                  # warmup_ratio=warmup_ratio,\n",
        "                                  group_by_length=True,                     # Group sequences into batches with same length\n",
        "                                  lr_scheduler_type=lr_scheduler_type,\n",
        "                                  report_to=\"tensorboard\"\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e6d30af7cae"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(model=model,\n",
        "                     args=training_args,\n",
        "                     peft_config=peft_config,\n",
        "                     train_dataset=hf_dataset,\n",
        "                     processing_class=tokenizer,\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ee44cd057afa"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ed259298f68b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "148"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "294ef180b291"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 1}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 24:59, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.581900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.224500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.967200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.154800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.941500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.078100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.981500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.025800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.897000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.017000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.802200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.902100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.936700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.037900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.816500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.083700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.743400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.984600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.811000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.966900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.802600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.042400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.708000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.915200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.666700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.913100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>0.663700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.863700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>0.641200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.883200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>0.694100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>0.588200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.949100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>0.674700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.875200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>0.635000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.783500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>0.659000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.886500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1025</td>\n",
              "      <td>0.688900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.840300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1075</td>\n",
              "      <td>0.745900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.714500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>0.716300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.885100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1175</td>\n",
              "      <td>0.694600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.854000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1225</td>\n",
              "      <td>0.749700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.850900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1275</td>\n",
              "      <td>0.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.824100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1325</td>\n",
              "      <td>0.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.749800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1375</td>\n",
              "      <td>0.611700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.906700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1425</td>\n",
              "      <td>0.509800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.784100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1475</td>\n",
              "      <td>0.634900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.904100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1525</td>\n",
              "      <td>0.637300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.861400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1575</td>\n",
              "      <td>0.621300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.782500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1625</td>\n",
              "      <td>0.556800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.869100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1675</td>\n",
              "      <td>0.620900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.778200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>0.521900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.923000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1775</td>\n",
              "      <td>0.616000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.840300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1825</td>\n",
              "      <td>0.520800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.806700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1875</td>\n",
              "      <td>0.694200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.875100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1925</td>\n",
              "      <td>0.623100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.775000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1975</td>\n",
              "      <td>0.632900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.787400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=0.8115992393493653, metrics={'train_runtime': 1502.325, 'train_samples_per_second': 1.331, 'train_steps_per_second': 1.331, 'total_flos': 1577734916802048.0, 'train_loss': 0.8115992393493653, 'epoch': 1.0})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "299e97061dc7"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bd44bffb2a60"
      },
      "outputs": [],
      "source": [
        "# save model to the local folder\n",
        "\n",
        "trainer.model.save_pretrained('finetuned_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ebb88892d5fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del model\n",
        "del trainer\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI0I71t5Jfxw"
      },
      "source": [
        "#### ***Merging Weights of Lora Config with Base model and Pushing to huggingfacehub models***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f7a2c6e7794"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "af22fc644e5d"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(base_model,r'/content/finetuned_model')           # This path is only for google colab\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# reloading tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "606e3c4bcdb5"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "\n",
        "locale.preferred_encoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "071e4478c67e"
      },
      "outputs": [],
      "source": [
        "name = \"shiv-am-04/gemma2-2b-SQL\"\n",
        "\n",
        "! huggingface-cli login\n",
        "\n",
        "model.push_to_hub(name, check_pr=True)\n",
        "\n",
        "tokenizer.push_to_hub(name,check_pr=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "[Gemma_2]Finetune_with_LORA.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
