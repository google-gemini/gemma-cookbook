{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83f33e2bb29544ddb2dfd8da62c9ca8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db04c0b220f84625b68581379ff6a95c",
              "IPY_MODEL_d6f5abc0de6841aebac973d50cdbc631",
              "IPY_MODEL_ca6385ae58554e5486f320aa001b018f"
            ],
            "layout": "IPY_MODEL_740697bec9b9483ebb194a49b7f702d3"
          }
        },
        "db04c0b220f84625b68581379ff6a95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48a9f96f16124e06b53cd9fcd42792ab",
            "placeholder": "​",
            "style": "IPY_MODEL_eef59bfa4aeb44a287d846b78e993c59",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d6f5abc0de6841aebac973d50cdbc631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fdccaed60614cc7af6288a98e70ceac",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca0b010dd3854ce785845d060873e5d7",
            "value": 2
          }
        },
        "ca6385ae58554e5486f320aa001b018f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db1b9a7ff70e4ccc97b7679804bc8705",
            "placeholder": "​",
            "style": "IPY_MODEL_8f3d6cd3ed874cca8f2e3c8e9590629e",
            "value": " 2/2 [00:03&lt;00:00,  1.80s/it]"
          }
        },
        "740697bec9b9483ebb194a49b7f702d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a9f96f16124e06b53cd9fcd42792ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eef59bfa4aeb44a287d846b78e993c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fdccaed60614cc7af6288a98e70ceac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca0b010dd3854ce785845d060873e5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db1b9a7ff70e4ccc97b7679804bc8705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f3d6cd3ed874cca8f2e3c8e9590629e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2024 Google LLC."
      ],
      "metadata": {
        "id": "dxP3r4mvQRnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "uU6msr-hQZGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma - 3 Function calling.\n",
        "\n",
        "Google released Gemma-3 its new variant that comes with Multimodality, Multilingual support upto 140 global languages, 128K context window and also Agentic capabilites like structured responses and function calling. In this notebook, we will explore how to use Function calling with open source models from HuggingFace via Gemma 3 4B-it model\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1SAdO-wGO-oKzk64BvYPpaaSrGH5SujcG?usp=sharing\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "HI6jQZyLQouk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "### Select the Colab runtime\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **▾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "### Gemma-3 setup\n",
        "\n",
        "To complete this tutorial, you'll first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "* Get access to Gemma on kaggle.com.\n",
        "* Select a Colab runtime with sufficient resources to run\n",
        "  the Gemma 3 4B-it model.\n",
        "* Generate and configure a Kaggle username and an API key as Colab secrets.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment.\n"
      ],
      "metadata": {
        "id": "8zdBd9FqRNYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies\n",
        "\n",
        "Run the cell below to install all the required dependencies.\n",
        "\n",
        "- Transformers: Latest version that supports Gemma3\n",
        "- Accelerate\n",
        "- DuckDuck Go Search - for function calling"
      ],
      "metadata": {
        "id": "UPG9Tt-xRXnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XpQ4yzBIHGbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "hX82hPriIM3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckduckgo-search"
      ],
      "metadata": {
        "id": "E8WujGTadPcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma 3\n",
        "\n",
        "Google has released Gemma-3, a successor to its earlier Gemma models, focusing on open-source accessibility and multimodal vision capabilities. The model is designed to handle complex tasks with extended context lengths, support for multiple languages, and vision capabilities, making it suitable for a wide range of applications.\n",
        "\n",
        "Gemma-3 has gained attention for outperforming notable models like DeepSeek V3, o3-mini, and Llama 3 405B, with extended context lengths, multilingual support, and multimodal capabilities.\n",
        "\n",
        "- Extended context length from 32K to 128K using ROPE scaling (from 10K to 1M) on global self-attention layers.\n",
        "- Available in 1B, 4B, 12B, and 27B sizes (Vision capability included, except for 1B, with bi-directional attention).\n",
        "- Supports an extensive range of 140 global languages.\n",
        "- Additional Features: Supports function calling, where the model can generate code or instructions to call external functions, and structured outputs, enabling it to produce data in specific formats like JSON or tables."
      ],
      "metadata": {
        "id": "a_KJh4F1Rjav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model weights"
      ],
      "metadata": {
        "id": "xUSjniskUsyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, Gemma3ForConditionalGeneration"
      ],
      "metadata": {
        "id": "us9dCp87GgVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-3-4b-it\""
      ],
      "metadata": {
        "id": "0p-2edQBR4af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
        "    model_id, device_map=\"auto\"\n",
        ").eval()\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "83f33e2bb29544ddb2dfd8da62c9ca8d",
            "db04c0b220f84625b68581379ff6a95c",
            "d6f5abc0de6841aebac973d50cdbc631",
            "ca6385ae58554e5486f320aa001b018f",
            "740697bec9b9483ebb194a49b7f702d3",
            "48a9f96f16124e06b53cd9fcd42792ab",
            "eef59bfa4aeb44a287d846b78e993c59",
            "2fdccaed60614cc7af6288a98e70ceac",
            "ca0b010dd3854ce785845d060873e5d7",
            "db1b9a7ff70e4ccc97b7679804bc8705",
            "8f3d6cd3ed874cca8f2e3c8e9590629e"
          ]
        },
        "id": "LISsLCCiGUVm",
        "outputId": "e12e702c-eeca-469f-86c5-85ef5cccb3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83f33e2bb29544ddb2dfd8da62c9ca8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "bnzOHRPtoQut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling Gemma\n",
        "\n",
        "Function calling is a feature that allows large language models to interact with external systems, APIs, and tools in a structured way. This feature enables LLMs to not just generate text responses but to invoke specific functions when appropriate based on user queries. At its core, function calling involves describing functions to the LLM and having it intelligently return the function to call along with necessary arguments for particular tasks. For example, an LLM can be configured to retrieve stock price data by calling a specific function when a user asks about the current price of a particular stock."
      ],
      "metadata": {
        "id": "i47OYekFdM4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def search(query:str):\n",
        "  \"\"\"\n",
        "  search results to the user query\n",
        "\n",
        "  Args:\n",
        "      query: user prompt to fetch search results\n",
        "  \"\"\"\n",
        "  req = DDGS()\n",
        "  response = req.text(query,max_results=4)\n",
        "  context = \"\"\n",
        "  for result in response:\n",
        "    context += result['body']\n",
        "  return context"
      ],
      "metadata": {
        "id": "UgU2fCoycyhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [search]"
      ],
      "metadata": {
        "id": "CQiUJlCBdb8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"which teams played Cricket ICC Champions Trophy 2025 finals and who won?\" # this is a realtime query, this event occured on March 9th 2025"
      ],
      "metadata": {
        "id": "R3z_9TbvufF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define prompt for function calling\n",
        "\n",
        "Function Calling follows these steps:\n",
        "\n",
        "- Your application sends a prompt to the LLM along with function definitions\n",
        "- The LLM analyzes the prompt and decides whether to respond directly or use defined functions\n",
        "- If using functions, the LLM generates structured arguments for the function call\n",
        "- Your application receives the function call details and executes the actual function\n",
        "- The function results are sent back to the LLM\n",
        "- The LLM provides a final response incorporating the function results"
      ],
      "metadata": {
        "id": "jHQrvzvRR7n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": \"\"\"\n",
        "        You are an expert search assistant. Use `search` to get the most accurate details.\n",
        "        At each turn, if you decide to invoke any of the function(s), it should be wrapped with ```tool_code```. The python methods described below are imported and available,\n",
        "        you can only use defined methods. The generated code should be readable and efficient. The response to a method will be wrapped in ```tool_output``` use it to call more tools or generate a helpful, friendly response.\n",
        "        When using a ```tool_call``` think step by step why and how it should be used.\n",
        "\n",
        "        The following Python methods are available:\n",
        "        \\`\\`\\`python\n",
        "        def search(query:str):\n",
        "          \"\n",
        "          search results to the user query\n",
        "\n",
        "          Args:\n",
        "             query: user prompt to fetch search results\n",
        "          \"\n",
        "        \\`\\`\\`\n",
        "\n",
        "        User: \\{user_message\\}\n",
        "        \"\"\"}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": query}\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "j4LiCaOodjyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor.apply_chat_template(\n",
        "            conversation,\n",
        "            tools=tools,\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True,\n",
        "            tokenize=True,\n",
        "            return_tensors=\"pt\",\n",
        ").to(model.device, dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "nh51Dr6Xde9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=512)"
      ],
      "metadata": {
        "id": "xNO25997ebXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = processor.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "XN32I1J2ltjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = output.split(\"model\")[-1]"
      ],
      "metadata": {
        "id": "moOkqbxAo0TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGF5oJyntu7C",
        "outputId": "d3c0433e-5c37-439f-bdc8-3ecb28403edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```tool_code\n",
            "search(query=\"Which teams played in the Cricket ICC Champions Trophy 2025 final and who won?\")\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute the function calling code"
      ],
      "metadata": {
        "id": "pldp3z7USS5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tool_call(text):\n",
        "    import io\n",
        "    from contextlib import redirect_stdout\n",
        "\n",
        "    pattern = r\"```tool_code\\s*(.*?)\\s*```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        code = match.group(1).strip()\n",
        "        # Capture stdout in a string buffer\n",
        "        f = io.StringIO()\n",
        "        with redirect_stdout(f):\n",
        "            result = eval(code)\n",
        "        output = f.getvalue()\n",
        "        r = result if output == '' else output\n",
        "        return f'```tool_output\\n{r}\\n```'''\n",
        "    return None"
      ],
      "metadata": {
        "id": "ni57IK_OpIya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword = extract_tool_call(response)"
      ],
      "metadata": {
        "id": "SRYVbLKdosaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "eznPlrIBpddw",
        "outputId": "bdb9be3f-5ae1-4ea7-bed5-d4ff9e6ddc58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"```tool_output\\nThe ninth edition of the Champions Trophy 2025 saw India being crowned as the winners on 9th March 2025 after they overcame New Zealand in the final. Several exceptional performers lit up the tournament with the bat and ball. The best of them made it to the Team of the Tournament. Here's what the side looks like: 1. Rachin Ravindra (New Zealand)The 2025 ICC Champions Trophy was the ninth edition of the ICC Champions Trophy, a quadrennial ODI cricket tournament organised by the International Cricket Council (ICC). In November 2021 as part of the 2024-2031 ICC men's hosts cycle, the ICC announced that the 2025 Champions Trophy would be played in Pakistan. [4]On 19 December 2024, following an agreement between Board of Control for ...Official ICC Champions Trophy, 2025 Cricket website - live matches, scores, news, highlights, commentary, rankings, videos and fixtures from the International Cricket Council. ... 2025. ICC announces Champions Trophy 2025 Team of the Tournament ... Kohli stars as India march into final | Champions Trophy 2025. 4 March, 2025. ICC Champions ...In a thrilling ICC Champions Trophy final at the Dubai International Cricket Stadium on March 9, 2025, India emerged victorious over New Zealand, clinching the title in a closely contested match ...\\n```\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": f\"Results from the search:{keyword} User_query : {query}\"}\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Ek9wlj3xuOnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = processor.apply_chat_template(\n",
        "            prompt,\n",
        "            tools=tools,\n",
        "            add_generation_prompt=True,\n",
        "            return_dict=True,\n",
        "            tokenize=True,\n",
        "            return_tensors=\"pt\",\n",
        ").to(model.device, dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "QB331y_TvN7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate(**final_prompt, max_new_tokens=512)"
      ],
      "metadata": {
        "id": "rcjE05aSvHEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.decode(response[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBcgbJjEvlW4",
        "outputId": "1bdecf35-2e4c-4e83-b8cd-321de8e8c0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user\n",
            "Results from the search:```tool_output\n",
            "The ninth edition of the Champions Trophy 2025 saw India being crowned as the winners on 9th March 2025 after they overcame New Zealand in the final. Several exceptional performers lit up the tournament with the bat and ball. The best of them made it to the Team of the Tournament. Here's what the side looks like: 1. Rachin Ravindra (New Zealand)The 2025 ICC Champions Trophy was the ninth edition of the ICC Champions Trophy, a quadrennial ODI cricket tournament organised by the International Cricket Council (ICC). In November 2021 as part of the 2024-2031 ICC men's hosts cycle, the ICC announced that the 2025 Champions Trophy would be played in Pakistan. [4]On 19 December 2024, following an agreement between Board of Control for ...Official ICC Champions Trophy, 2025 Cricket website - live matches, scores, news, highlights, commentary, rankings, videos and fixtures from the International Cricket Council. ... 2025. ICC announces Champions Trophy 2025 Team of the Tournament ... Kohli stars as India march into final | Champions Trophy 2025. 4 March, 2025. ICC Champions ...In a thrilling ICC Champions Trophy final at the Dubai International Cricket Stadium on March 9, 2025, India emerged victorious over New Zealand, clinching the title in a closely contested match ...\n",
            "``` User_query : which teams played Cricket ICC Champions Trophy 2025 finals and who won?\n",
            "model\n",
            "According to the search results:\n",
            "\n",
            "*   **Teams:** India and New Zealand\n",
            "*   **Winner:** India won the final on March 9, 2025.\n"
          ]
        }
      ]
    }
  ]
}