{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH85BOCo7YYk"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9tQNAByc7U9g"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsh9bFNyHJWE"
      },
      "source": [
        "## Fine-Tuning Gemma for Retrieval-Augmented Generation with JORA\n",
        "\n",
        "Scaling Large Language Models (LLMs) for retrieval-based tasks, particularly in Retrieval-Augmented Generation (RAG), poses significant memory challenges, especially when fine-tuning extensive prompt sequences.\n",
        "\n",
        "[Gemma](https://ai.google.dev/gemma) is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
        "\n",
        "Existing open-source libraries support full-model inference and fine-tuning across multiple GPUs but often fall short in efficiently distributing parameters required for retrieved context. To address this limitation, [JORA](https://github.com/aniquetahir/JORA) introduced a novel framework for Parameter-Efficient Fine-Tuning (PEFT) of Llama/Gemma models using distributed training, leveraging [JAX](https://jax.readthedocs.io/en/latest/). This framework uniquely utilizes JAX's just-in-time (JIT) compilation and tensor-sharding for efficient resource management, enabling accelerated fine-tuning with reduced memory requirements. This advancement significantly improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources.\n",
        "\n",
        "The experiments demonstrate more than **12x improvement in runtime** compared to [Hugging Face](https://huggingface.co/docs/transformers/en/main_classes/trainer)/[DeepSpeed](https://github.com/microsoft/DeepSpeed) implementations with four GPUs while consuming less than half the VRAM per GPU. All in all, this library improves the scalability and feasibility of fine-tuning LLMs for complex RAG applications, even on systems with limited GPU resources.\n",
        "\n",
        "In this tutorial, you will understand the end-to-end process of fine-tuning a [Gemma](https://github.com/google/gemma) model using JORA and converting the trained model back to the [Hugging Face](https://huggingface.co/) format for inference.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/Finetune_with_JORA.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFX5p8a4HJWF"
      },
      "source": [
        "## Setup Environment\n",
        "\n",
        "Before you begin, ensure you have access to a Colab notebook with GPU runtime enabled. Go to **Runtime** > **Change runtime type** and select the right **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Select the Colab runtime\n",
        "To complete this tutorial, you'll need to have a [**Colab Pro/Pro+**](https://colab.research.google.com/signup) runtime with sufficient resources to run the Gemma model. In this case, you can use a **A100** GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **â–¾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **A100 GPU**.\n",
        "\n",
        "### **Kaggle** Gemma setup\n",
        "\n",
        "To complete this tutorial and download and fine-tune using the necessary Kaggle Gemma Flax models, you'll first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "* Get access to Gemma on kaggle.com.\n",
        "* Select a Colab runtime with sufficient resources to run\n",
        "  the Gemma model.\n",
        "* You'll generate and configure a Kaggle username and an API key as Colab secrets later in the guide.\n",
        "\n",
        "### **Hugging Face** Gemma setup\n",
        "\n",
        "You'll also be logging in to Hugging Face Hub to download the exact Gemma model used while fine-tuning. So, let's get you set up with Gemma:\n",
        "\n",
        "1. **Hugging Face Account:**  If you don't already have one, you can create a free Hugging Face account by clicking [here](https://huggingface.co/join).\n",
        "2. **Gemma Model Access:** Head over to the [Gemma model page](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b) and accept the usage conditions.\n",
        "3. **Colab with Gemma Power:**  For this tutorial, you'll need a Colab runtime with enough resources to handle the Gemma model. Choose an appropriate runtime when starting your Colab session.\n",
        "4. **Hugging Face Token:**  Generate a Hugging Face access (preferably `write` permission) token by clicking [here](https://huggingface.co/settings/tokens). This token will come in handy later.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY2kGtsyYpHF"
      },
      "source": [
        "### Configure your credentials\n",
        "\n",
        "Add your **Hugging Face** (`HF_TOKEN`) and **Kaggle** tokens (`KAGGLE_USERNAME` and `KAGGLE_KEY`) to the Colab Secrets manager to securely store them.\n",
        "\n",
        "1. Open your Google Colab notebook and click on the ğŸ”‘ Secrets tab in the left panel. <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
        "2. Create a new secret with the name `HF_TOKEN`.\n",
        "3. Copy/paste your token key into the Value input box of `HF_TOKEN`.\n",
        "4. Toggle the button on the left to allow notebook access to the secret.\n",
        "5. Repeat it for the Kaggle secrets with names `KAGGLE_USERNAME` and `KAGGLE_KEY`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-1PYEuJuJyN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# Disable progress bar to prevent verbose logging by kagglehub\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Pj2Y9EHJWF"
      },
      "source": [
        "### Clone **JORA** and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryPIut33HJWF"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'JORA'...\n",
            "remote: Enumerating objects: 299, done.\u001b[K\n",
            "remote: Counting objects: 100% (299/299), done.\u001b[K\n",
            "remote: Compressing objects: 100% (216/216), done.\u001b[K\n",
            "remote: Total 299 (delta 151), reused 203 (delta 71), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (299/299), 6.99 MiB | 7.89 MiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n",
            "/content/JORA\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gemma (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Clone the JORA repository and install the requirements\n",
        "!git clone https://github.com/aniquetahir/JORA.git\n",
        "%cd JORA\n",
        "!pip install -q -e .\n",
        "\n",
        "# Install google-deepmind/gemma as it's a required dependency for JORA\n",
        "!pip install -q git+https://github.com/google-deepmind/gemma.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMa4gjjvM_8N"
      },
      "source": [
        "### Import the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHfyqXc7SuBc"
      },
      "outputs": [],
      "source": [
        "# Patch JORA's initialisation.py file to be compatible with the latest JAX version\n",
        "\n",
        "!sed -i \"s/jax\\.config\\.update('jax_default_matmul_precision', *jax\\.lax\\.Precision\\.HIGHEST)/jax.config.update('jax_default_matmul_precision', 'bfloat16')/\" jora/lib/proc_init_utils/initialisation.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8einQYITNKxR"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import jax\n",
        "import jora\n",
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import snapshot_download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gEJeEXWHJWG"
      },
      "source": [
        "## Download the Gemma Model\n",
        "\n",
        "Now, you can download the Gemma model using `kagglehub`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIEg5GXJHJWH"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "decb63b2502b4557a44ed5ac373996c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/ocdbt.process_0/manifest.0000000000000002...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/d/76bc26e743d2a486ffd9322898d08435...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/ocdbt.process_0/manifest.ocdbt...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/ocdbt.process_0/manifest.0000000000000003...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/ocdbt.process_0/d/b09eb83b9b0c3b46e65c59c98c25f6bc...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/ocdbt.process_0/d/82f86d35f73a7dd200794e77ea112419...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/ocdbt.process_0/manifest.0000000000000001...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/ocdbt.process_0/d/f55163dd3a5e6bfe83651d679b783cdb...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/checkpoint...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/manifest.0000000000000001...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/_METADATA...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/manifest.ocdbt...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/2b-it/manifest.0000000000000002...\n",
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/Flax/1.1-2b-it/1/download/tokenizer.model...\n",
            "GEMMA_PATH: /root/.cache/kagglehub/models/google/gemma/Flax/1.1-2b-it/1\n"
          ]
        }
      ],
      "source": [
        "VARIANT = \"1.1-2b-it\"\n",
        "GEMMA_PATH = kagglehub.model_download(f'google/gemma/Flax/{VARIANT}')\n",
        "\n",
        "print('GEMMA_PATH:', GEMMA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnB1B5TxHJWH"
      },
      "source": [
        "**Note:** By default, `kagglehub` stores the model in the `~/.cache/kagglehub` directory.\n",
        "\n",
        "Verify that JAX recognizes the GPU devices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsLysmfPHJWH"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CudaDevice(id=0)]\n"
          ]
        }
      ],
      "source": [
        "print(jax.devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB8Xxe9RHJWH"
      },
      "source": [
        "## Configure JORA and Prepare the Dataset\n",
        "\n",
        "Here, you'll configure the Gemma model and also the training process for **LoRA** fine-tuning.\n",
        "\n",
        "In order to fine-tune Gemma, you will use the **Alpaca** dataset. Ensure you have the dataset file `alpaca_data_cleaned.json` in the appropriate directory. You can download it from [here](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data_cleaned.json) or use the one that's bundled in the repository. For demonstration purposes, let's use the bundled one.\n",
        "\n",
        "**Credits:** [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)\n",
        "\n",
        "The `generate_alpaca_dataset` function is used to generate the dataset from an Alpaca format JSOB file. This helps with instruct format training since the dataset processing, tokenization, and batching is handled by the library. Alternatively, torch `Dataset` and `DataLoader` can be used for custom datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyWf7EVpHJWH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing data...\n"
          ]
        }
      ],
      "source": [
        "# Configure the model and training parameters\n",
        "config = jora.ParagemmaConfig(\n",
        "    # Feel free to tweak these parameters\n",
        "    N_EPOCHS=1,\n",
        "    LORA_R=8,\n",
        "    # Note: The `LORA_DROPOUT` parameter is currently not configurable.\n",
        "    # https://github.com/aniquetahir/JORA?tab=readme-ov-file#contributing\n",
        "    LORA_ALPHA=16,\n",
        "    LR=1e-5,\n",
        "    BATCH_SIZE=2,\n",
        "    N_ACCUMULATION_STEPS=8,\n",
        "    GEMMA_MODEL_PATH=GEMMA_PATH,\n",
        "    MAX_SEQ_LEN=512,\n",
        "    # Set `MODEL_VERSION` to '2b-it'\n",
        "    # https://github.com/aniquetahir/JORA/blob/master/jora/lib/gemma/common.py#L282\n",
        "    MODEL_VERSION='2b-it'\n",
        ")\n",
        "\n",
        "# Path to the Alpaca dataset\n",
        "dataset_path = 'jora/alpaca_data_cleaned.json'\n",
        "\n",
        "# Generate the dataset\n",
        "dataset = jora.generate_alpaca_dataset_gemma(\n",
        "    dataset_path, 'train', config,\n",
        "    split_percentage=0.2,\n",
        "    alpaca_mix=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMDYXOKKHJWH"
      },
      "source": [
        "The `ParagemmaConfig` class is used to set up the configuration for training while `generate_alpaca_dataset_gemma` processes the dataset, handles tokenization, and prepares it for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0_iDvXsIRSx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ParagemmaConfig(GEMMA_MODEL_PATH='/root/.cache/kagglehub/models/google/gemma/Flax/1.1-2b-it/1', MODEL_VERSION='2b-it', NUM_SHARDS=None, LORA_R=8, LORA_ALPHA=16, LORA_DROPOUT=0.05, LR=1e-05, BATCH_SIZE=2, N_ACCUMULATION_STEPS=8, MAX_SEQ_LEN=512, N_EPOCHS=1, SEED=420, CACHE_SIZE=30)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FguwkPZHJWH"
      },
      "source": [
        "## Fine-tune Gemma with **JORA**\n",
        "\n",
        "Now, you can proceed to fine-tune the model using the `train_lora_gemma` function which initiates the fine-tuning process using LoRA (Low-Rank Adaptation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwr5ObmKHJWH"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">Epochs...</span>                                               <span style=\"color: #729c1f; text-decoration-color: #729c1f\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"> total_loss: 13896.38671875, loss: 1.2124241590499878</span>                                                        \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Batches...</span>                                              <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #800080; text-decoration-color: #800080\">  0%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\"> total_loss: 13848.3291015625, loss: 0.8443454504013062</span>                                                      \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31mEpochs...\u001b[0m                                               \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
              "\u001b[31m total_loss: 13896.38671875, loss: 1.2124241590499878\u001b[0m                                                        \n",
              "\u001b[32mBatches...\u001b[0m                                              \u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
              "\u001b[32m total_loss: 13848.3291015625, loss: 0.8443454504013062\u001b[0m                                                      \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Path to the trained LoRA weights\n",
        "checkpoint_path = 'checkpoints'\n",
        "jora.train_lora_gemma(config, dataset, checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3D9iMEfHJWH"
      },
      "source": [
        "Checkpoints will be saved in the folder specified by `checkpoint_path`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcbH1MCNHJWH"
      },
      "source": [
        "## Convert the model to the **Hugging Face Format**\n",
        "\n",
        "After fine-tuning, you need to convert the trained model to the Hugging Face format for compatibility with the Hugging Face ecosystem so that you can easily run inference later.\n",
        "\n",
        "**Usage:**\n",
        "\n",
        "```python\n",
        "lorize_huggingface(HUGGINGFACE_PATH, JAX_PATH, SAVE_PATH, gemma=True)\n",
        "```\n",
        "\n",
        "- **HUGGINGFACE_PATH**: Path to the Hugging Face Gemma model (the base model before fine-tuning).\n",
        "- **JAX_PATH**: Path to the LoRA merged parameters (the trained LoRA weights).\n",
        "- **SAVE_PATH**: Path to save the updated Hugging Face Gemma fine-tuned model.\n",
        "- **gemma**: Flag indicating you're working with a Gemma model.\n",
        "\n",
        "First, specify the paths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0TaWlbFHJWI"
      },
      "outputs": [],
      "source": [
        "# Specify the repository\n",
        "repo_id = \"google/gemma-1.1-2b-it\"\n",
        "local_dir = 'pretrained'\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=repo_id,\n",
        "    local_dir=local_dir,\n",
        "    revision=\"main\",\n",
        "    ignore_patterns=['*.gguf']\n",
        ")\n",
        "\n",
        "HUGGINGFACE_PATH = local_dir  # Path to the base Hugging Face model\n",
        "JAX_PATH = 'checkpoints/jax_lora_final.pickle'  # Path to the trained LoRA JAX weights\n",
        "SAVE_PATH = 'gemma-ft'  # Path to save the converted Hugging Face model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zDE8A6KHJWI"
      },
      "source": [
        "Then, run the converter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUpmZXMhUWNz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fa85ea51e444c85a42a502c6e0faebb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model loaded\n",
            "model saved to gemma-ft\n"
          ]
        }
      ],
      "source": [
        "from jora.hf.__main__ import lorize_huggingface\n",
        "\n",
        "lorize_huggingface(HUGGINGFACE_PATH, JAX_PATH, SAVE_PATH, gemma=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fimTt20qHJWI"
      },
      "source": [
        "- The `jora.hf` module converts the JAX-trained model back to the Hugging Face format.\n",
        "- It merges the LoRA weights with the original model parameters.\n",
        "- The converted model is saved in the specified `SAVE_PATH`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH91P3gRHJWI"
      },
      "source": [
        "## Load the Model and Generate Text\n",
        "\n",
        "Finally, you can load the converted model using Hugging Face's Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "romkTs-LO7P6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c399062cc6d4e7291617737cc574f7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_PATH)\n",
        "model = AutoModelForCausalLM.from_pretrained(SAVE_PATH, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acWcwfqIHJWI"
      },
      "source": [
        "Here, the tokenizer and model are first loaded and then the model is moved to the GPU.\n",
        "\n",
        "Generate text using the model. To do this, you'll use the Alpaca prompt format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C32TYrMgHJWI"
      },
      "outputs": [],
      "source": [
        "# Define the Alpaca prompt template\n",
        "alpaca_prompt = \"\"\"\\\n",
        "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Function to generate response\n",
        "def generate_response(instruction, input_text=\"\", max_new_tokens=384):\n",
        "    prompt = alpaca_prompt.format(instruction, input_text)\n",
        "    device = \"cuda\"\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(inputs, max_new_tokens=max_new_tokens)\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn3CAvUZln5k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Make a prediction about what will happen in the next paragraph.\n",
            "\n",
            "### Input:\n",
            "Mary had been living in the small town for many years and had never seen anything like what was coming.\n",
            "\n",
            "### Response:\n",
            "Mary will be surprised by what she will see.\n"
          ]
        }
      ],
      "source": [
        "generate_response(\n",
        "    instruction=\"Make a prediction about what will happen in the next paragraph.\",\n",
        "    input_text=\"Mary had been living in the small town for many years and had never seen anything like what was coming.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5wj7ZsDhPHH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify a suitable <verb> in the following sentence.\n",
            "\n",
            "### Input:\n",
            "Cat <verb> in the garden.\n",
            "\n",
            "### Response:\n",
            "Cat sat in the garden.\n"
          ]
        }
      ],
      "source": [
        "generate_response(\n",
        "    instruction=\"Identify a suitable <verb> in the following sentence.\",\n",
        "    input_text=\"Cat <verb> in the garden.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3jL-Z8CtqgP"
      },
      "source": [
        "## Push the model to your Hugging Face Hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM84Ti3r02Tz"
      },
      "source": [
        "Optionally, Hugging Face allows to you easily store trained models in their hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIDWBva0_SX4"
      },
      "outputs": [],
      "source": [
        "# Note: The token needs to have \"write\" permission\n",
        "#       You can check it here:\n",
        "#       https://huggingface.co/settings/tokens\n",
        "# Uncomment and run this if you wish to publish the model to Hugging Face Hub\n",
        "# model.push_to_hub(\"my-gemma-finetuned-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3jKeiM_HJWJ"
      },
      "source": [
        "In this tutorial, you have learnt how to fine-tune a Gemma model using JORA and convert it to the Hugging Face model format for inference. By leveraging JAX's JIT compilation and tensor-sharding capabilities, you can achieve efficient resource management, enabling accelerated fine-tuning with reduced memory requirements."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Finetune_with_JORA.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
