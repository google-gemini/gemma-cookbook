# AI-Native Task Analysis Report

## 1. Feature Ideas (Vision-stage or Greenfield)

### Idea 1: Gemma Accessibility Assistant
**Concept:** A mobile or web application leveraging PaliGemma (Vision-Language Model) or Gemma 3 (Multimodal) to assist visually impaired users.
**Core Features:**
- **Scene Description:** Users can point their camera at a scene, and the app describes it in natural language.
- **Text-to-Speech Navigation:** Reads signs, menus, and documents aloud.
- **Object Recognition:** Identifies objects and their relative positions ("The coffee mug is to your left").
**Differentiation:** Existing demos show capability, but a focused product with a specialized UI/UX for accessibility is a significant gap.

### Idea 2: Automated Code Review Bot (CI/CD Integration)
**Concept:** A GitHub Action or GitLab Runner that uses CodeGemma to automatically review Pull Requests.
**Core Features:**
- **Refactoring Suggestions:** Identifies code smells and suggests refactors (e.g., simplifying loops, improving variable names).
- **Bug Detection:** Spots potential logic errors or security vulnerabilities.
- **Style Enforcement:** Ensures code adheres to project-specific style guides beyond simple linting.
**Differentiation:** Moves beyond local IDE assistants (like `personal-code-assistant`) to a centralized, enforced workflow tool.

### Idea 3: Interactive Storytelling Engine
**Concept:** A dynamic "Choose Your Own Adventure" game engine powered by Gemma 2 or 3 for creative writing.
**Core Features:**
- **Dynamic Plot Generation:** Story arcs evolve based on user choices, not pre-scripted paths.
- **NPC Interaction:** Characters have persistent memories and personalities generated by the model.
- **World Building:** The model generates descriptions of locations, items, and lore on the fly.
**Differentiation:** Showcases Gemma's creative capabilities in a consumer-facing entertainment product, distinct from the utility-focused demos.

---

## 2. Tech Debt Identification

**File:** `Demos/spoken-language-tasks/k-mail-replier/k_mail_replier/models/gemma.py`

**Issues:**
1.  **Fragile Regex:** The `extract_substring` function uses a regex:
    ```python
    match = re.search(r".*<end_of_turn>\n<start_of_turn>model\n(.*)<end_of_turn>", text, re.DOTALL)
    ```
    This relies on exact string matching of tags which might change across model versions or quantization levels. It lacks robustness.
2.  **Commented-out Code:** There are lines like `#gemma.summary()` and `#gemma.compile(...)` left in the production code.
3.  **Hardcoded Paths:** Loading LoRA weights from `./weights/` assumes the execution context is always the app root.

**File:** `Demos/personal-code-assistant/gemma-web-service/gemma_service/gemma_service_main.py`

**Issue:**
-   **Blocking Async:** The endpoint is defined as `async def process_text(...)`, but calls `gemma_model(request.text)` which is a synchronous, CPU-bound operation (model inference).
    ```python
    @app.post("/gemma_request/")
    async def process_text(request: Request):
        response_text = gemma_model(request.text)  # BLOCKS EVENT LOOP
    ```
    This blocks the entire FastAPI event loop, preventing other requests from being handled concurrently.

---

## 3. Duplicate Logic Findings

**Pattern:** Gemma Model Initialization & Response Trimming

**Locations:**
- `Demos/personal-code-assistant/gemma-web-service/gemma_service/gemma_model.py`
- `Demos/spoken-language-tasks/k-mail-replier/k_mail_replier/models/gemma.py`

**Duplication:**
1.  **Environment Setup:** Both files load `.env`, check for `KAGGLE_USERNAME`/`KAGGLE_KEY`, and set `KERAS_BACKEND` to `jax`.
2.  **Model Loading:** Both use `keras_nlp.models.GemmaCausalLM.from_preset(...)`.
3.  **Prompt Engineering:** Both construct prompts with `<start_of_turn>user...`.
4.  **Response Processing:** Both have logic to strip these tags from the output (`trim_response` vs `extract_substring`).

---

## 4. Clustering & Refactoring Suggestions

**Project:** `Demos/Gemma-on-Cloudrun`
**File:** `converter.go`

**Suggestion:**
The `converter.go` file contains a mix of:
- Google GenAI to OpenAI API request conversion.
- OpenAI to Google GenAI response conversion.
- Stream handling logic.
- Hardcoded model mapping.

**Refactor:**
Cluster these into a `pkg/converter` or `pkg/adapter` package with separate files:
- `request_mapper.go`: Handles `GenerateContentRequest` -> `ChatCompletionRequest`.
- `response_mapper.go`: Handles `ChatCompletionResponse` -> `GenerateContentResponse`.
- `stream_handler.go`: Handles the SSE stream transformation.
- `models.go`: Stores the model mapping constants.

---

## 5. Scope Issue: "Consolidate Gemma Initialization"

**Title:** Refactor Demos to use shared `gemma_utils` library

**Problem:**
Multiple Python demos (`personal-code-assistant`, `k-mail-replier`) duplicate the logic for initializing the Gemma model, checking Kaggle credentials, and processing model output. This violates DRY and makes updates (e.g., changing the default model version) tedious.

**Scope:**
1.  Create a new directory `Common/python/gemma_utils/`.
2.  Extract `initialize_model`, `create_model_instance` (factory), and `trim_response` logic into `gemma_utils.py`.
3.  Update `Demos/personal-code-assistant` to import from `Common`.
4.  Update `Demos/spoken-language-tasks` to import from `Common`.
5.  Add unit tests for `gemma_utils`.

---

## 6. Componentizing Logic (Module Conversion)

**Task:** Convert `initialize_model` into a reusable module.

**Proposed `gemma_utils.py`:**

```python
import os
import keras_nlp
from dotenv import load_dotenv

def setup_gemma_env():
    load_dotenv()
    if not os.getenv('KAGGLE_USERNAME') or not os.getenv('KAGGLE_KEY'):
        raise ValueError("KAGGLE_USERNAME and KAGGLE_KEY must be set in .env")
    os.environ["KERAS_BACKEND"] = "jax"
    os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.00"

def get_gemma_model(preset="gemma2_instruct_2b_en", lora_path=None):
    setup_gemma_env()
    gemma = keras_nlp.models.GemmaCausalLM.from_preset(preset)
    if lora_path:
         gemma.backbone.enable_lora(rank=4)
         gemma.backbone.load_lora_weights(lora_path)
    return gemma

def trim_response(text):
    # robust extraction logic
    if "<start_of_turn>model\n" in text:
        text = text.split("<start_of_turn>model\n")[-1]
    return text.replace("<end_of_turn>", "").strip()
```

---

## 7. Parallel Processing Refactor

**Task:** Refactor `process_text` in `gemma_service_main.py` to be amenable to parallel processing (fix blocking issue).

**Refactor:**
FastAPI runs `def` (sync) endpoints in a threadpool, but `async def` endpoints on the main loop. Since `gemma_model` is blocking, we should either:
1.  Remove `async` from the definition.
2.  Or use `run_in_executor`.

**Recommended Fix (Option 2 - Explicit Async):**

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

# Create a dedicated thread pool for model inference
# to avoid starving the default executor.
model_executor = ThreadPoolExecutor(max_workers=1)

@app.post("/gemma_request/")
async def process_text(request: Request):
    """
    Processes the input text asynchronously without blocking the event loop.
    """
    loop = asyncio.get_running_loop()
    # Offload the blocking model call to the thread pool
    response_text = await loop.run_in_executor(
        model_executor,
        gemma_model,
        request.text
    )

    response = Response(text=response_text)
    return response
```
This ensures the server remains responsive to other requests (like health checks or other I/O) while the model computes.
