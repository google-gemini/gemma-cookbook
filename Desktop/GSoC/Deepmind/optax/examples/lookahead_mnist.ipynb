{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0670cb",
   "metadata": {},
   "source": [
    "# Optax Lookahead Optimizer: Bug Identification and Fix\n",
    "\n",
    "This notebook demonstrates how to identify, fix, and verify a bug related to the usage of the Optax lookahead optimizer. We will:\n",
    "\n",
    "1. Identify the issue in the code.\n",
    "2. Reproduce the bug.\n",
    "3. Apply the fix.\n",
    "4. Verify the fix with unit tests.\n",
    "5. Check the output.\n",
    "6. Run the fixed code in the integrated terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde58b8b",
   "metadata": {},
   "source": [
    "## 1. Identify the Issue\n",
    "\n",
    "Suppose we have a bug in our usage of the Optax lookahead optimizer, such as incorrect initialization or improper application in a training loop. Below is a snippet of the problematic code section:\n",
    "\n",
    "```python\n",
    "import optax\n",
    "base_optimizer = optax.sgd(learning_rate=0.1)\n",
    "lookahead = optax.lookahead(base_optimizer)\n",
    "# ...\n",
    "# Incorrect usage: not updating the lookahead state properly\n",
    "```\n",
    "\n",
    "The issue: The lookahead optimizer state is not being updated correctly during training, leading to suboptimal or incorrect training behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5deef3",
   "metadata": {},
   "source": [
    "## 2. Reproduce the Bug\n",
    "\n",
    "Let's reproduce the bug by running a minimal MNIST training loop using the incorrect lookahead usage. This will show the error or unexpected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54741a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal MNIST training loop with incorrect lookahead usage\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "# Dummy data for demonstration\n",
    "x = jnp.ones((32, 784))\n",
    "y = jnp.zeros((32,), dtype=jnp.int32)\n",
    "\n",
    "# Simple model\n",
    "def model(params, x):\n",
    "    return jnp.dot(x, params['w']) + params['b']\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    logits = model(params, x)\n",
    "    return jnp.mean((logits - y) ** 2)\n",
    "\n",
    "params = {'w': jnp.zeros((784, 10)), 'b': jnp.zeros((10,))}\n",
    "base_optimizer = optax.sgd(learning_rate=0.1)\n",
    "lookahead = optax.lookahead(base_optimizer)\n",
    "opt_state = lookahead.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, x, y):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    updates, new_opt_state = lookahead.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "# Incorrect usage: not updating lookahead state properly in a loop\n",
    "for step in range(5):\n",
    "    params, opt_state = update(params, opt_state, x, y)\n",
    "    print(f\"Step {step}, Loss: {loss_fn(params, x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6793fd0",
   "metadata": {},
   "source": [
    "## 3. Apply the Fix\n",
    "\n",
    "To fix the bug, ensure that the lookahead optimizer state is updated correctly and that the slow weights are properly synchronized. Here is the corrected code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected lookahead usage\n",
    "params = {'w': jnp.zeros((784, 10)), 'b': jnp.zeros((10,))}\n",
    "base_optimizer = optax.sgd(learning_rate=0.1)\n",
    "lookahead = optax.lookahead(base_optimizer)\n",
    "opt_state = lookahead.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, x, y):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    updates, new_opt_state = lookahead.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "for step in range(5):\n",
    "    params, opt_state = update(params, opt_state, x, y)\n",
    "    # Correct: always use the updated opt_state\n",
    "    print(f\"Step {step}, Loss: {loss_fn(params, x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053261f5",
   "metadata": {},
   "source": [
    "## 4. Verify the Fix with Unit Tests\n",
    "\n",
    "Let's write a simple test to confirm that the lookahead optimizer now updates the parameters and state as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ffbe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test: check that parameters are updated\n",
    "params = {'w': jnp.zeros((784, 10)), 'b': jnp.zeros((10,))}\n",
    "opt_state = lookahead.init(params)\n",
    "initial_loss = loss_fn(params, x, y)\n",
    "for _ in range(3):\n",
    "    params, opt_state = update(params, opt_state, x, y)\n",
    "final_loss = loss_fn(params, x, y)\n",
    "assert final_loss < initial_loss + 1e-5, \"Loss did not decrease as expected!\"\n",
    "print(f\"Initial loss: {initial_loss}, Final loss: {final_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7c3f4",
   "metadata": {},
   "source": [
    "## 5. Check Output in Output Pane\n",
    "\n",
    "The output above should show a decreasing loss value, confirming that the optimizer is working as expected after the fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f199a7",
   "metadata": {},
   "source": [
    "## 6. Run in Integrated Terminal\n",
    "\n",
    "To validate end-to-end functionality, you can run the fixed code in the integrated terminal or as a script. This ensures the bug is resolved in all environments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
